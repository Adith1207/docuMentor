This project combines:

CodeScribe â†’ generates Python docstrings & inline comments using Gemini API.

LocalRQA â†’ Retrieval-Augmented Question Answering on local .py and .txt files using CodeBERT embeddings + phi-2 generator (runs locally).

ğŸ“‚ Project Structure
localrqa_codescribe/
â”œâ”€ app/
â”‚   â””â”€ server.py              # FastAPI server
â”œâ”€ src/
â”‚   â”œâ”€ models.py              # Handles Gemini + phi-2 + embeddings
â”‚   â”œâ”€ comment_generator.py   # Code commenting (Gemini)
â”‚   â”œâ”€ rag_model.py           # RQA logic
â”‚   â”œâ”€ build_index.py         # Build FAISS embeddings index
â”‚   â””â”€ utils.py
â”œâ”€ data/                      # Put your .py / .txt files here for RQA
â”œâ”€ embeddings/                # Auto-created FAISS index + vectors
â”œâ”€ .env                       # Gemini API key & model (not in git)
â”œâ”€ requirements.txt
â””â”€ README.md

âš™ï¸ Setup

Clone repo

git clone https://github.com/yourname/yourrepo.git
cd localrqa_codescribe


Create virtualenv & install deps

python -m venv .venv
source .venv/bin/activate   # (Windows: .venv\Scripts\activate)
pip install -r requirements.txt


Set up .env file
Create a file named .env in the backend root:

GEMINI_API_KEY=your_google_api_key_here
CODESCRIBE_MODEL=models/gemini-2.5-pro


ğŸ”‘ Get your Gemini API key from Google AI Studio
.

(Optional) Build embeddings for LocalRQA
Add .py or .txt files into the data/ folder, then run:

python src/build_index.py

â–¶ï¸ Running the Server

Run with uvicorn:

python -m uvicorn app.server:app --reload --host 0.0.0.0 --port 8000


Server runs at â†’ http://127.0.0.1:8000

Example endpoints:

POST /comment-text â†’ Generate comments for code string.

POST /comment-file â†’ Upload .py file, get commented version.

POST /ask â†’ Ask questions about your local documents.

GET /ping â†’ Health check.

ğŸŒ Frontend (Optional)

If youâ€™re using the included React frontend (App.jsx), start it with:

npm install
npm run dev


It will connect to the FastAPI backend on port 8000.
